01/05/2025 09:34:00 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:49, 16.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:32<00:32, 16.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:50<00:16, 16.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 11.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:53<00:00, 13.32s/it]
01/05/2025 09:34:55 - INFO - src.dp.dp_generator -   Using HF for inference
Inferencing DP:   0%|          | 0/199 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
/home/FYP/mohor001/.conda/envs/creativity/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Inferencing DP:   1%|          | 1/199 [02:35<8:33:46, 155.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
slurmstepd: error: *** JOB 11803 ON TC1N07 CANCELLED AT 2025-01-05T09:39:02 ***
